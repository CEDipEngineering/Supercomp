{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequential DNA analysis\n",
    "\n",
    "The problem of dna subsequence matching is a very difficult problem in computer science, belonging to a category of problems known as Nondeterministic Polynomial, or NP. These problems are often of exponential time complexity, and therefore can take enormous amounts of time to analyse completely.\n",
    "\n",
    "Therefore, some techniques can be applied to reach suboptimal results in a more attainable timeframe. As such, some of these techniques are known as Heuristics. These allow for an algorithm to exploit known characteristics of the problem, in order to search through the space of possibilities in a more intelligent fashion.\n",
    "\n",
    "In this sense, there are some heuristics and techniques that differ in their approach, into what can be described as two categories; Exploration, wherein the algorithm seeks to try newer solutions whenever possible, so as to explore the space of possibilities in a broader sense; and Exploitation, which revolves around making use of known characteristics of the problem in order to improve upon any given solution, often iteratively, so as to try and reach a good answer. Some of the algorithms below lean more on one or another, as will be discussed ahead.  \n",
    "\n",
    "In this report, I delve into four different approaches to solve this problem. As they stand:\n",
    "- Smith Waterman Heuristic:\n",
    "\n",
    "    The Smith Waterman heuristic makes use of a Matrix to analyze every combination more efficiently, and then allows for back-tracking through said matrix for reconstructing the substring which produced the highest score. This algorithm makes use of mostly exploitation, and very little exploration.\n",
    "\n",
    "- Local Search\n",
    "\n",
    "    The Local Search algorithm will take a random substring of one of the sequences, and then try to match it to a series of random subsequences in the other sequence, so as to try varying combinations, and hopefully achieve a close solution to the real best fit. This algorithm makes use of mostly exlporation, and very little exploitation.\n",
    "\n",
    "- Exaustive Search\n",
    "\n",
    "    Exaustive Search consists on a naive implementation of the basic concept of \"Search every combination to make sure you know the best one\". It is pure exploration, as it needs no information about the structure of the problem to create the space of possibilities.\n",
    "\n",
    "- Bounded Exaustive Search\n",
    "\n",
    "    Bounded Exaustive Search is a more balanced approach, which searches through possibilities in a certain order, so as to maximize the chances of getting the best score possible, as quickly as possible, and once no improvement is available from that point, computations are terminated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'in'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/user/Supercomp/DNAProject/analyser.ipynb Cell 2'\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B10.103.3.160/home/user/Supercomp/DNAProject/analyser.ipynb#ch0000001vscode-remote?line=7'>8</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmpl_toolkits\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmplot3d\u001b[39;00m \u001b[39mimport\u001b[39;00m Axes3D\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B10.103.3.160/home/user/Supercomp/DNAProject/analyser.ipynb#ch0000001vscode-remote?line=8'>9</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mrandom\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B10.103.3.160/home/user/Supercomp/DNAProject/analyser.ipynb#ch0000001vscode-remote?line=9'>10</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39manalyser\u001b[39;00m \u001b[39mimport\u001b[39;00m Test\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.103.3.160/home/user/Supercomp/DNAProject/analyser.ipynb#ch0000001vscode-remote?line=11'>12</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m/home/user/Supercomp/DNAProject/cache.pickle\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m f:\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.103.3.160/home/user/Supercomp/DNAProject/analyser.ipynb#ch0000001vscode-remote?line=12'>13</a>\u001b[0m     time_dict \u001b[39m=\u001b[39m pickle\u001b[39m.\u001b[39mload(f)\n",
      "File \u001b[0;32m~/Supercomp/DNAProject/analyser.py:15\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     <a href='file:///home/user/Supercomp/DNAProject/analyser.py?line=10'>11</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mitertools\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mit\u001b[39;00m\n\u001b[1;32m     <a href='file:///home/user/Supercomp/DNAProject/analyser.py?line=12'>13</a>\u001b[0m \u001b[39m# Run with nohup python3 analyser.py & > nohup.txt\u001b[39;00m\n\u001b[0;32m---> <a href='file:///home/user/Supercomp/DNAProject/analyser.py?line=14'>15</a>\u001b[0m test_list \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mmap\u001b[39m(\u001b[39mlambda\u001b[39;00m x: \u001b[39m'\u001b[39m\u001b[39min/\u001b[39m\u001b[39m'\u001b[39m\u001b[39m+\u001b[39mx, os\u001b[39m.\u001b[39;49mlistdir(\u001b[39m'\u001b[39;49m\u001b[39min\u001b[39;49m\u001b[39m'\u001b[39;49m)))\n\u001b[1;32m     <a href='file:///home/user/Supercomp/DNAProject/analyser.py?line=16'>17</a>\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mTest\u001b[39;00m():\n\u001b[1;32m     <a href='file:///home/user/Supercomp/DNAProject/analyser.py?line=17'>18</a>\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, size_a, size_b, time):\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'in'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import time\n",
    "import subprocess\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import random\n",
    "from analyser import Test\n",
    "\n",
    "with open('/home/user/Supercomp/DNAProject/cache.pickle', 'rb') as f:\n",
    "    time_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing\n",
    "\n",
    "Tests were generated at runtime, then directed to Stdin for each executable using the script [analyser.py](analyser.py), which after all executions were terminated, dumped a dictionary of times and input sizes to a .pickle file for access here. It is important to note, that to remove time variance for the algorithms, each execution repeated three times, and an average was then collected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis\n",
    "\n",
    "Below, 4 3D graphs are created to analyse the time progression of all 4 algorithms. These all have the sizes of the sequences on the x and y axes, and the z axis represents the time in milliseconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exaustive Search\n",
    "experiment_list_exaustive = list(map(lambda x: x.__dict__ ,time_dict['Sequential/ExaustiveSearch/main']))\n",
    "experiment_list_exaustive\n",
    "congregated = {}\n",
    "for k in list(experiment_list_exaustive[0]):\n",
    "    congregated[k] = []\n",
    "for d in experiment_list_exaustive:\n",
    "    for k,v in d.items():\n",
    "        congregated[k].append(v)\n",
    "df = pd.DataFrame(congregated).sort_values('time',ascending=False).reset_index().drop('index', axis=1)\n",
    "fig = plt.figure(figsize=(14,29))\n",
    "ax = fig.add_subplot(121, projection='3d')\n",
    "ax.plot_trisurf(df['size_a'], df['size_b'], df['time'], cmap='plasma')\n",
    "ax.set_xlabel('Size of sequence A')\n",
    "ax.set_ylabel('Size of sequence B')\n",
    "ax.set_zlabel('Time (ms)')\n",
    "ax.set_title('Exaustive Search')\n",
    "\n",
    "\n",
    "# Local Search\n",
    "experiment_list_exaustive = list(map(lambda x: x.__dict__ ,time_dict['Sequential/LocalSearch/main']))\n",
    "experiment_list_exaustive\n",
    "congregated = {}\n",
    "for k in list(experiment_list_exaustive[0]):\n",
    "    congregated[k] = []\n",
    "for d in experiment_list_exaustive:\n",
    "    for k,v in d.items():\n",
    "        congregated[k].append(v)\n",
    "df = pd.DataFrame(congregated).sort_values('time',ascending=False).reset_index().drop('index', axis=1)\n",
    "ax2 = fig.add_subplot(222, projection='3d')\n",
    "ax2.plot_trisurf(df['size_a'], df['size_b'], df['time'], cmap='plasma')\n",
    "ax2.set_xlabel('Size of sequence A')\n",
    "ax2.set_ylabel('Size of sequence B')\n",
    "ax2.set_zlabel('Time (ms)')\n",
    "ax2.set_title('Local Search')\n",
    "\n",
    "\n",
    "# SmithWaterman Search\n",
    "experiment_list_exaustive = list(map(lambda x: x.__dict__ ,time_dict['Sequential/SmithWaterman/main']))\n",
    "experiment_list_exaustive\n",
    "congregated = {}\n",
    "for k in list(experiment_list_exaustive[0]):\n",
    "    congregated[k] = []\n",
    "for d in experiment_list_exaustive:\n",
    "    for k,v in d.items():\n",
    "        congregated[k].append(v)\n",
    "df = pd.DataFrame(congregated).sort_values('time',ascending=False).reset_index().drop('index', axis=1)\n",
    "ax3 = fig.add_subplot(221, projection='3d')\n",
    "ax3.plot_trisurf(df['size_a'], df['size_b'], df['time'], cmap='plasma')\n",
    "ax3.set_xlabel('Size of sequence A')\n",
    "ax3.set_ylabel('Size of sequence B')\n",
    "ax3.set_zlabel('Time (ms)')\n",
    "ax3.set_title('Smith Waterman Search')\n",
    "\n",
    "\n",
    "# Bounded Exaustive Search\n",
    "experiment_list_exaustive = list(map(lambda x: x.__dict__ ,time_dict['Sequential/BoundedExaustiveSearch/main']))\n",
    "experiment_list_exaustive\n",
    "congregated = {}\n",
    "for k in list(experiment_list_exaustive[0]):\n",
    "    congregated[k] = []\n",
    "for d in experiment_list_exaustive:\n",
    "    for k,v in d.items():\n",
    "        congregated[k].append(v)\n",
    "df = pd.DataFrame(congregated).sort_values('time',ascending=False).reset_index().drop('index', axis=1)\n",
    "ax4 = fig.add_subplot(122, projection='3d')\n",
    "ax4.plot_trisurf(df['size_a'], df['size_b'], df['time'], cmap='plasma')\n",
    "ax4.set_xlabel('Size of sequence A')\n",
    "ax4.set_ylabel('Size of sequence B')\n",
    "ax4.set_zlabel('Time (ms)')\n",
    "ax4.set_title('Bounded Exaustive Search')\n",
    "\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " As may be seen on the above graphs, local search took a constant time, which might seem as a complexity of O(1), however it is worth noting that this does not guarantee optimal output, and only checks a select portion of the sequence, yielding often quite poor results.\n",
    "\n",
    " Smith Waterman is also similar in that it is very fast, altough not optimal, however there does seem to be some increase in time based on input size (albeit non-linear). It is interesting to note, that for inputs of size less than 250x250, it seems to be rather constant, and this might be due to the OS switching thread execution to other tasks after a certain amount of time has passed for the larger inputs (purely speculative).  \n",
    "\n",
    " The naive Exaustive search is rather slow, clocking in at around 50 seconds for some of the larger inputs (not shown in graph), but seems to perform reasonably well for smaller sizes. It does however guarantee the optimal solution.\n",
    "\n",
    " The alternative Bounded Exaustive Search did seem to produce the same results as the naive Exaustive search in my tests, but I'm not confident it is also optimal. Whatever the case may be, it is around 100x faster than its naive counterpart, and still produces great results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In-depth analysis of algorithms\n",
    "\n",
    "## SmithWaterman\n",
    "\n",
    "The Smith Waterman heuristic seems to present a good balance between speed and accuracy. It is, however, not possible to analyze the score proximity to the other algorithms, beacause of a critical difference in strategy. Smith Waterman considers possible gaps in sequences, such that sequences like 'CTG' and 'CAG' get matched not only as themselves, but also 'C-G' to 'C-G' and '-TG' to C-G'. When comparing large sequences, this can produce large differences and create high-scoring long sequences, that don't match other searches that were implemented.\n",
    "\n",
    "The main point of contention with Smith Waterman is its space complexity (creating a $n \\times m$ matrix is quite space-intensive), as is the case with most dynamic programming solutions.\n",
    "\n",
    "## Local Search\n",
    "\n",
    "As presented, the Local Search algorithm produced very fast times across the board, which would be very impressive, if it had achieved anywhere near the correct solution. As implemented, it simply can't try that many possibilities, and as such is not likely to reach a good result.\n",
    "\n",
    "It does not take any additional space given a larger input, and does not seem to increase in time given the change in input size, so it does not seem to need any improvements in terms of speed or space complexity.\n",
    "\n",
    "## Exaustive Search\n",
    "\n",
    "The implementation of this algorithm is very straightforward. First we iterate over each string, acumulating all possible substrings in a set (set is used to avoid repetition, especially on smaller sizes). Then, loop through the first set, checking every possibility in the other set, and keeping track of the best score. If the selected substrings are of different size, check every possible combination of matching size.\n",
    "\n",
    "This is obviously very repetitive and slow, which is why this takes 50sec on an input of size 100x100. It does also have O(n) space complexity to store the additional sets of substrings, which does not help its case. In general this a poor implementation of a slow algorithm.\n",
    "\n",
    "## Bounded Exaustive Search\n",
    "\n",
    "The implementation of this algorithm starts from a size of $n=min(size(a), size(b))$ and then progressively reduces n as it searches every substring of size n on the larger sequence. As soon as it finds a new high score that is also a full match ($Score = 2 \\cdot n$, assuming a value for matches of 2), it stops its execution. This is because any string smaller than n, can never score more than $2n$.\n",
    "\n",
    "This algorithm has a time complexity of O($n^3$), the most executed code is to score a pair of sequences.\n",
    "\n",
    "# Parallel computing\n",
    "\n",
    "## Multi-Core LocalSearch\n",
    "\n",
    "The next implementation was of a LocalSearch, in a multi-core environment (4-core CPU). In order to adapt all the code, many changes to implementation details had to be accounted for, and so as to keep the comparison more fair, the actual algorithm implementation was made in a separate .hpp header file, and merely imported and run a few dozen times (each time producing a different result, since this depended on a pseudo-random algorithm). This means that both sequential and parallel programs execute identical code, and only differ in terms of threading(the parallel implementation launches and executes multiple threads simultaneously, whereas in the sequential, experiments attempted one by one, by a single thread)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sequential Search\n",
    "experiment_list = list(map(lambda x: x.__dict__ ,time_dict['Parallel/LocalSearch/main_seq']))\n",
    "experiment_list\n",
    "congregated = {}\n",
    "for k in list(experiment_list[0]):\n",
    "    congregated[k] = []\n",
    "for d in experiment_list:\n",
    "    for k,v in d.items():\n",
    "        congregated[k].append(v)\n",
    "df = pd.DataFrame(congregated).sort_values('time',ascending=False).reset_index().drop('index', axis=1)\n",
    "fig = plt.figure(figsize=(14,29))\n",
    "ax = fig.add_subplot(121, projection='3d')\n",
    "ax.plot_trisurf(df['size_a'], df['size_b'], df['time'], cmap='plasma')\n",
    "ax.set_xlabel('Size of sequence A')\n",
    "ax.set_ylabel('Size of sequence B')\n",
    "ax.set_zlabel('Time (ms)')\n",
    "ax.set_title('Sequential Local Search (Baseline)')\n",
    "\n",
    "# Parallel Local Search\n",
    "experiment_list = list(map(lambda x: x.__dict__ ,time_dict['Parallel/LocalSearch/main_par']))\n",
    "experiment_list\n",
    "congregated = {}\n",
    "for k in list(experiment_list[0]):\n",
    "    congregated[k] = []\n",
    "for d in experiment_list:\n",
    "    for k,v in d.items():\n",
    "        congregated[k].append(v)\n",
    "df = pd.DataFrame(congregated).sort_values('time',ascending=False).reset_index().drop('index', axis=1)\n",
    "ax = fig.add_subplot(122, projection='3d')\n",
    "ax.plot_trisurf(df['size_a'], df['size_b'], df['time'], cmap='plasma')\n",
    "ax.set_xlabel('Size of sequence A')\n",
    "ax.set_ylabel('Size of sequence B')\n",
    "ax.set_zlabel('Time (ms)')\n",
    "ax.set_title('Parallel Local Search');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen in the graphs, the CPU multithreaded approach is only marginally faster, possibly due to the overhead in creating and launching new stacks, as well as all the stack manipulation in the calling of subroutines inside each thread. Also, 4 threads is not that big of a number, and with a more powerful multithreaded system, this coul be faster.\n",
    "\n",
    "## Many-Core Exaustive Search (GPU)\n",
    "\n",
    "In the GPU, many calculations can be performed simultaneously, which can lead to massive performance gains for large input sizes. Unfortunately, at the scale these tests would show a significant difference, the available time-frame for this project would become rather unachievable, and so below we compare the naive exaustive search from above with two different implementations of GPU parallelization (detailed after the graphs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sequential Search\n",
    "experiment_list = list(map(lambda x: x.__dict__ ,time_dict['Sequential/ExaustiveSearch/main']))\n",
    "experiment_list\n",
    "congregated = {}\n",
    "for k in list(experiment_list[0]):\n",
    "    congregated[k] = []\n",
    "for d in experiment_list:\n",
    "    for k,v in d.items():\n",
    "        congregated[k].append(v)\n",
    "df = pd.DataFrame(congregated).sort_values('time',ascending=False).reset_index().drop('index', axis=1)\n",
    "fig = plt.figure(figsize=(18,18))\n",
    "ax = fig.add_subplot(211, projection='3d')\n",
    "ax.plot_trisurf(df['size_a'], df['size_b'], df['time'], cmap='plasma')\n",
    "ax.set_xlabel('Size of sequence A')\n",
    "ax.set_ylabel('Size of sequence B')\n",
    "ax.set_zlabel('Time (ms)')\n",
    "ax.set_title('Sequential Exaustive Search (Baseline)')\n",
    "\n",
    "# Parallel Search\n",
    "experiment_list = list(map(lambda x: x.__dict__ ,time_dict['Parallel/GPU/main']))\n",
    "experiment_list\n",
    "congregated = {}\n",
    "for k in list(experiment_list[0]):\n",
    "    congregated[k] = []\n",
    "for d in experiment_list:\n",
    "    for k,v in d.items():\n",
    "        congregated[k].append(v)\n",
    "df = pd.DataFrame(congregated).sort_values('time',ascending=False).reset_index().drop('index', axis=1)\n",
    "ax = fig.add_subplot(223, projection='3d')\n",
    "ax.plot_trisurf(df['size_a'], df['size_b'], df['time'], cmap='plasma')\n",
    "ax.set_xlabel('Size of sequence A')\n",
    "ax.set_ylabel('Size of sequence B')\n",
    "ax.set_zlabel('Time (ms)')\n",
    "ax.set_title('Parallel Exaustive Search with Smith Waterman');\n",
    "\n",
    "# Parallel Search\n",
    "experiment_list = list(map(lambda x: x.__dict__ ,time_dict['Parallel/Hybrid/main']))\n",
    "experiment_list\n",
    "congregated = {}\n",
    "for k in list(experiment_list[0]):\n",
    "    congregated[k] = []\n",
    "for d in experiment_list:\n",
    "    for k,v in d.items():\n",
    "        congregated[k].append(v)\n",
    "df = pd.DataFrame(congregated).sort_values('time',ascending=False).reset_index().drop('index', axis=1)\n",
    "ax = fig.add_subplot(224, projection='3d')\n",
    "ax.plot_trisurf(df['size_a'], df['size_b'], df['time'], cmap='plasma')\n",
    "ax.set_xlabel('Size of sequence A')\n",
    "ax.set_ylabel('Size of sequence B')\n",
    "ax.set_zlabel('Time (ms)')\n",
    "ax.set_title('Parallel Exaustive Search with Smith Waterman (Hybrid)');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen above, both parallel GPU implementations were quite slow. In fact, compared to the sequential implementation for these input sizes, it can be hundreds of times slower. Before we can analyse the reasons behind that fact, we should first elaborate on the implementation details of each new algorithm above.\n",
    "\n",
    "On the left, labeled as \"Parallel Exaustive Search with Smith Waterman\", the scoring of each pair of subsequences is done with Smith Waterman, and the parallel part is implemented exclusively on the Smith Waterman scoring. In particular, this parallelism was encapsulated in a function, which sends all the data to the GPU, every time, and returns an individual score.\n",
    "\n",
    "On the right, labeled as \"Parallel Exaustive Search with Smith Waterman (Hybrid)\", the previous algorithm was further optimized, initially by only creating the data for the sequences and actual value vectors once, and then only manipulating the pointers in order to test combinations (this reduces the amount of data on the data bus dramatically, which produces a very large increase in performance), and after that was implemented, OpenMP CPU parallelization of transform calls was used in order to fill the GPU processing queue, which is meant to reduce GPU down time as much as possible (for instance, while waiting for the next task, the GPU would be idle in the previous program, but now it always has several tasks lined up).\n",
    "\n",
    "As mentioned, the main bottleneck for this process is the transfer of data to the GPU, which is very large in the first implementation. However, with some clever tricks and iterator pointer manipulations, the same algorithm can be sped up significantly, as seen above. The Hybrid pointers algorithm achieved about half the time, at an input twice as large (in both dimensions), which would be a speedup of about 8x over the slow naive implementation.\n",
    "\n",
    "During this project, there was an attempt to run the naive algorithm using inputs larger than 50x50. Specifically, it was set up to run up to 200x200. Unfortunately, after being left running with nohup (Ubuntu program to prevent hangup shutdown) over 40 hours, it still did not complete its execution, and as such, the data was left at 50x50.\n",
    "\n",
    "## Conclusions and considerations\n",
    "\n",
    "The DNA sequence matching problem is far from simple, and can be extremely challenging at times. Many different heuristics and algorithms not shown here exist, such as a Genetic Algorithm or some machine learning solutions. For small subsequences the bounded exaustive search implemented above produced very good results in very little time, however, it scaled quite poorly for inputs that are very large. Smith Waterman implementations are very fast and produce very good results, but cannot guarantee optimal solutions, and use up large amounts of memory. And finally, Local Search implementations can be rather fast, however they often produce not great results, due to relying mostly on exploration.\n",
    "\n",
    "For larger sizes of input, the parallel implementations may be noticeably faster than the sequential alternatives, however they both take amounts of time well outside the scope of this project, and as such were not directly validated.\n",
    "\n",
    "Finally, all implementations are available in the GitHub repository, under a directory structure, with clear makefiles and simple usability for anyone interested in running these examples for themselves."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
